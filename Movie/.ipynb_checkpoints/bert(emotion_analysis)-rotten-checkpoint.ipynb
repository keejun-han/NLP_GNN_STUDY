{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c770538e-13b1-4954-ba23-822b574380b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from utils import (\n",
    "    tokenizer_setting,\n",
    "    preprocessing,\n",
    "    GPU_setting,\n",
    "    hyperparmeter_setting,\n",
    "    flat_accuracy,\n",
    "    format_time,\n",
    "    initial_setting,\n",
    "    run_train,\n",
    "    run_test,\n",
    "    convert_input_data,\n",
    "    test_sentence_unit,\n",
    "    test_sentence_many\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f02b1-10d6-4b02-ae3d-b6442005cbc3",
   "metadata": {},
   "source": [
    "### 1. Initial Setting\n",
    "- Load the data\n",
    "- Split train, test data\n",
    "- Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9c73c3-f594-4455-8e55-ae8fa90514c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint.pth.tar',\n",
       " 'model_new.pth',\n",
       " 'model_save',\n",
       " 'test.txt',\n",
       " 'train.txt',\n",
       " 'val.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './../../data/rotten_tomato/emotion_analysis_data/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db002eaf-ab84-4612-a514-90b6823486ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['default']\n",
    "train_df = pd.read_csv(os.path.join(path + 'train.txt'), names=header, encoding='utf-8')\n",
    "test_df = pd.read_csv(os.path.join(path + 'test.txt'), names=header, encoding='utf-8')\n",
    "val_df = pd.read_csv(os.path.join(path + 'val.txt'), names=header, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ff6b97-f19b-4004-88c7-8b2b05d6a779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated;sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy;anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             default\n",
       "0                    i didnt feel humiliated;sadness\n",
       "1  i can go from feeling so hopeless to so damned...\n",
       "2  im grabbing a minute to post i feel greedy wro...\n",
       "3  i am ever feeling nostalgic about the fireplac...\n",
       "4                         i am feeling grouchy;anger"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ec651e-f549-4f4a-942c-922dd897081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [train_df, test_df, val_df]\n",
    "\n",
    "for df in df_list:\n",
    "    df['content'] = df.default.str.split(';').str[0]\n",
    "    df['emotion'] = df.default.str.split(';').str[1]\n",
    "    df.drop('default', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03bfd474-e3b3-4d50-b2c8-83c70d5f149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  emotion\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73c5011-2869-4068-9114-9a8a74320b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger', 'fear', 'joy', 'love', 'sadness', 'surprise'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_df['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "018088ce-8e20-4f03-9ace-9249c21d005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_value_dict = {'anger':0, 'fear':1, 'joy':2, 'love':3, 'sadness':4, 'surprise':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa2bab6d-215a-4cd3-a07d-85b80fddce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [train_df, test_df, val_df]\n",
    "\n",
    "for df in df_list:\n",
    "    df = df.replace({'emotion': change_value_dict}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d84a87-d913-447b-84b0-a23d07254465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling quite sad and sorry for myself but ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like i am still looking at a blank canv...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel like a faithful servant</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am just feeling cranky and blue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i can have for a treat or if i am feeling festive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  emotion\n",
       "0  im feeling quite sad and sorry for myself but ...        4\n",
       "1  i feel like i am still looking at a blank canv...        4\n",
       "2                     i feel like a faithful servant        3\n",
       "3                  i am just feeling cranky and blue        0\n",
       "4  i can have for a treat or if i am feeling festive        2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89abd98e-2a8a-4c4f-ad11-c834e1a84bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "print(len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf79055-824e-4d2f-8fe4-7b68cedf1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['content']\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = test_df['content']\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "X_val = val_df['content']\n",
    "y_val = val_df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c93d42e-6847-4c7f-9c0d-aab2fe2a5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_setting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2d1ab-edf5-4008-bbaf-72696b64909b",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "634e3b3c-c9d7-4a25-96d6-af4ca5d3b41b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = preprocessing(X_train, y_train, tokenizer, process_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8d80691-d280-4e9a-ac6b-6d58faa3b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = preprocessing(X_test, y_test, tokenizer, process_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "559ca9dd-8548-4453-b381-6c62dd58609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = preprocessing(X_val, y_val, tokenizer, process_type='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02560d9-c9e8-435f-9dcd-e4fe99a0ac08",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6564be32-43de-4b24-b6f1-23d49bb9e563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user\\anaconda3\\envs\\torch37\\lib\\site-packages\\torch\\cuda\\__init__.py:106: UserWarning: \n",
      "NVIDIA GeForce RTX 3080 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 3080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading took: 0:34:15\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels = 6) #label개수에 따라 변경\n",
    "model.cuda()\n",
    "print(\"  Loading took: {:}\".format(format_time(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56441de3-2d8c-4d91-b7c5-89f51084049f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device 0 : NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "device = GPU_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af1907ef-3070-45fa-97e6-3a21832b55d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-34dc5569d628>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparmeter_setting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer, epochs, total_steps, scheduler = hyperparmeter_setting(model, train_dataloader, lr=2e-5, eps=1e-8, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "934fb112-0be2-4ca4-9335-65dc4c451bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initial_setting(model, seed_val=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c759a-4f05-46da-97aa-524a52eed31f",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d944acc-8a08-4989-8093-c9e086ecfb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.76\n",
      "  Training epoch took: 0:09:59\n",
      "  Model Checkpoint Save\n",
      "file_path: ./../../data/rotten_tomato/emotion_analysis_data/checkpoint.pth.tar\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation took: 0:00:20\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:08:26\n",
      "  Model Checkpoint Save\n",
      "file_path: ./../../data/rotten_tomato/emotion_analysis_data/checkpoint.pth.tar\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:20\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:08:25\n",
      "  Model Checkpoint Save\n",
      "file_path: ./../../data/rotten_tomato/emotion_analysis_data/checkpoint.pth.tar\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:20\n",
      "\n",
      "Total took: 0:28:06\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = run_train(model, epochs, train_dataloader, validation_dataloader, optimizer, scheduler, device, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770509dc-eb47-47d1-9e53-299b609ecc3a",
   "metadata": {},
   "source": [
    "### 5. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32564ce8-42f9-4a84-8d87-d2e664880b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path+\"model_new.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2b77020-5bf3-443c-a377-5768943d46dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-854d305c3dd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"model_new.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(path+\"model_new.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a88ef561-c4f7-4783-98c8-bd13a84a9b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./../../data/rotten_tomato/emotion_analysis_data/model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./../../data/rotten_tomato/emotion_analysis_data/model_save/tokenizer_config.json',\n",
       " './../../data/rotten_tomato/emotion_analysis_data/model_save/special_tokens_map.json',\n",
       " './../../data/rotten_tomato/emotion_analysis_data/model_save/vocab.txt',\n",
       " './../../data/rotten_tomato/emotion_analysis_data/model_save/added_tokens.json')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = path + 'model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc65a0-0b7f-4a84-8595-852be17a5177",
   "metadata": {},
   "source": [
    "- Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a58ede2e-61cd-439d-9e99-c482e07cfd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = path + 'model_save/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "056a0d21-5da3-4bf0-a7fb-c78cf34b0f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./../../data/rotten_tomato/emotion_analysis_data/model_save/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d1ba6fd-2ba8-4790-8ac4-6caf560fd947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading took: 0:29:43\n"
     ]
    }
   ],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "start_time = time.time()\n",
    "model_new = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model_new.to(device)\n",
    "print(\"  Loading took: {:}\".format(format_time(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12a100bd-27a1-4795-983d-b2ec5b90846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a418ae3-e60c-4c7f-87d7-a0b65a585db0",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4239e0d0-cfd3-481c-b91f-ffa97fbabf63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.93\n",
      "Test took: 0:00:21\n"
     ]
    }
   ],
   "source": [
    "run_test(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f4e5e-56db-46b0-91d4-7ba225fa5e52",
   "metadata": {},
   "source": [
    "- 단일 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12a03897-f90f-4925-b8a4-95ca93a010f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0, 'fear': 1, 'joy': 2, 'love': 3, 'sadness': 4, 'surprise': 5}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "539a5aba-dde1-46a1-bdff-608a9f2c582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.4027362 -1.5702496  5.7927184 -1.575778  -1.0552726 -0.5253772]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Whether audiences will get behind The Lightning Thief is hard to predict. Overall, it's an entertaining introduction to a promising new world -- but will the consuming shadow of Potter be too big to break free of?\"\n",
    "logits = test_sentence_unit(model, device, tokenizer, [sentence])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab1034e0-a010-49a0-8eb6-489e84f6423c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9511262   0.41871074  0.8441826  -2.3129752   0.46622762 -2.3007205 ]]\n",
      "0\n",
      "  Loading took: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "sentence = \"i hate you\"\n",
    "logits = test_sentence_unit(model, device, tokenizer, [sentence])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))\n",
    "print(\"  Loading took: {:}\".format(format_time(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7783c3a2-7774-46fb-97f9-73b888992d44",
   "metadata": {},
   "source": [
    "- 여러 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1bec74c6-55b2-4e3e-9f57-2af65950875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = './../../data/rotten_tomato/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6aaa68f7-e1e4-48c8-bec6-9c7f545a2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(path2+'rotten_rating_review_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dfc681b8-57d9-4aa6-b30e-9402ab248875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>origin_index</th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>critic_name</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher_name</th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>943</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ben McEachen</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday Mail (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7242</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Nick Schager</td>\n",
       "      <td>False</td>\n",
       "      <td>Slant Magazine</td>\n",
       "      <td>Rotten</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Harry Potter knockoffs don't come more transpa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1046</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Bill Goodykoontz</td>\n",
       "      <td>True</td>\n",
       "      <td>Arizona Republic</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Percy Jackson isn't a great movie, but it's a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4895</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Jordan Hoffman</td>\n",
       "      <td>False</td>\n",
       "      <td>UGO</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Fun, brisk and imaginative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4517</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Jim Schembri</td>\n",
       "      <td>True</td>\n",
       "      <td>The Age (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Crammed with dragons, set-destroying fights an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  origin_index rotten_tomatoes_link       critic_name  \\\n",
       "0      943         0             3            m/0814255      Ben McEachen   \n",
       "1     7242         0             6            m/0814255      Nick Schager   \n",
       "2     1046         0             7            m/0814255  Bill Goodykoontz   \n",
       "3     4895         0             8            m/0814255    Jordan Hoffman   \n",
       "4     4517         0             9            m/0814255      Jim Schembri   \n",
       "\n",
       "   top_critic           publisher_name review_type  review_score review_date  \\\n",
       "0       False  Sunday Mail (Australia)       Fresh          0.70  2010-02-09   \n",
       "1       False           Slant Magazine      Rotten          0.25  2010-02-10   \n",
       "2        True         Arizona Republic       Fresh          0.70  2010-02-10   \n",
       "3       False                      UGO       Fresh          0.70  2010-02-10   \n",
       "4        True      The Age (Australia)       Fresh          0.60  2010-02-10   \n",
       "\n",
       "                                      review_content  \n",
       "0  Whether audiences will get behind The Lightnin...  \n",
       "1  Harry Potter knockoffs don't come more transpa...  \n",
       "2  Percy Jackson isn't a great movie, but it's a ...  \n",
       "3                         Fun, brisk and imaginative  \n",
       "4  Crammed with dragons, set-destroying fights an...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae6dc0ac-398e-470f-b623-7e1668e1d027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "752664"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "709e094a-80b6-4342-a392-d691b4b399bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = merged_df[['user_id', 'movie_id', 'review_score', 'review_content', 'review_type','review_date', 'critic_name', 'top_critic', 'publisher_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88911126-1819-4a5c-bea3-137055164c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_content</th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_date</th>\n",
       "      <th>critic_name</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>943</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>Ben McEachen</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday Mail (Australia)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7242</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Harry Potter knockoffs don't come more transpa...</td>\n",
       "      <td>Rotten</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Nick Schager</td>\n",
       "      <td>False</td>\n",
       "      <td>Slant Magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Percy Jackson isn't a great movie, but it's a ...</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Bill Goodykoontz</td>\n",
       "      <td>True</td>\n",
       "      <td>Arizona Republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Fun, brisk and imaginative</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Jordan Hoffman</td>\n",
       "      <td>False</td>\n",
       "      <td>UGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4517</td>\n",
       "      <td>0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>Crammed with dragons, set-destroying fights an...</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Jim Schembri</td>\n",
       "      <td>True</td>\n",
       "      <td>The Age (Australia)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  review_score  \\\n",
       "0      943         0          0.70   \n",
       "1     7242         0          0.25   \n",
       "2     1046         0          0.70   \n",
       "3     4895         0          0.70   \n",
       "4     4517         0          0.60   \n",
       "\n",
       "                                      review_content review_type review_date  \\\n",
       "0  Whether audiences will get behind The Lightnin...       Fresh  2010-02-09   \n",
       "1  Harry Potter knockoffs don't come more transpa...      Rotten  2010-02-10   \n",
       "2  Percy Jackson isn't a great movie, but it's a ...       Fresh  2010-02-10   \n",
       "3                         Fun, brisk and imaginative       Fresh  2010-02-10   \n",
       "4  Crammed with dragons, set-destroying fights an...       Fresh  2010-02-10   \n",
       "\n",
       "        critic_name  top_critic           publisher_name  \n",
       "0      Ben McEachen       False  Sunday Mail (Australia)  \n",
       "1      Nick Schager       False           Slant Magazine  \n",
       "2  Bill Goodykoontz        True         Arizona Republic  \n",
       "3    Jordan Hoffman       False                      UGO  \n",
       "4      Jim Schembri        True      The Age (Australia)  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "806cd8f6-6260-408e-b5df-01aac19fc8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "752664"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06e40408-62f6-4150-8943-dc1a3a0c653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eba10512-d642-4b5d-a904-40357aed2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_many(model, device, tokenizer, sentences):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 출력된 label 리스트\n",
    "    label_list = list() \n",
    "    \n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 문장을 입력 데이터로 변환\n",
    "    inputs, masks = convert_input_data(tokenizer, sentences)\n",
    "\n",
    "    # 데이터를 GPU에 넣음\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "    \n",
    "    test_data = TensorDataset(b_input_ids, b_input_mask)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - start_time)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask = batch\n",
    "    \n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():     \n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        # 로스 구함\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        label_list.append(list(pred_flat))\n",
    "    \n",
    "    # 이중 리스트를 단일 리스트로 변경\n",
    "    result = list(itertools.chain.from_iterable(label_list))    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce5f2b3b-9855-44bf-9f62-6f760a75a580",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  23,521.    Elapsed: 0:05:51.\n",
      "  Batch   200  of  23,521.    Elapsed: 0:06:22.\n",
      "  Batch   300  of  23,521.    Elapsed: 0:06:52.\n",
      "  Batch   400  of  23,521.    Elapsed: 0:07:23.\n",
      "  Batch   500  of  23,521.    Elapsed: 0:07:54.\n",
      "  Batch   600  of  23,521.    Elapsed: 0:08:25.\n",
      "  Batch   700  of  23,521.    Elapsed: 0:08:56.\n",
      "  Batch   800  of  23,521.    Elapsed: 0:09:27.\n",
      "  Batch   900  of  23,521.    Elapsed: 0:09:58.\n",
      "  Batch 1,000  of  23,521.    Elapsed: 0:10:29.\n",
      "  Batch 1,100  of  23,521.    Elapsed: 0:11:00.\n",
      "  Batch 1,200  of  23,521.    Elapsed: 0:11:31.\n",
      "  Batch 1,300  of  23,521.    Elapsed: 0:12:02.\n",
      "  Batch 1,400  of  23,521.    Elapsed: 0:12:33.\n",
      "  Batch 1,500  of  23,521.    Elapsed: 0:13:04.\n",
      "  Batch 1,600  of  23,521.    Elapsed: 0:13:35.\n",
      "  Batch 1,700  of  23,521.    Elapsed: 0:14:06.\n",
      "  Batch 1,800  of  23,521.    Elapsed: 0:14:36.\n",
      "  Batch 1,900  of  23,521.    Elapsed: 0:15:07.\n",
      "  Batch 2,000  of  23,521.    Elapsed: 0:15:38.\n",
      "  Batch 2,100  of  23,521.    Elapsed: 0:16:09.\n",
      "  Batch 2,200  of  23,521.    Elapsed: 0:16:41.\n",
      "  Batch 2,300  of  23,521.    Elapsed: 0:17:11.\n",
      "  Batch 2,400  of  23,521.    Elapsed: 0:17:42.\n",
      "  Batch 2,500  of  23,521.    Elapsed: 0:18:13.\n",
      "  Batch 2,600  of  23,521.    Elapsed: 0:18:45.\n",
      "  Batch 2,700  of  23,521.    Elapsed: 0:19:16.\n",
      "  Batch 2,800  of  23,521.    Elapsed: 0:19:47.\n",
      "  Batch 2,900  of  23,521.    Elapsed: 0:20:17.\n",
      "  Batch 3,000  of  23,521.    Elapsed: 0:20:48.\n",
      "  Batch 3,100  of  23,521.    Elapsed: 0:21:19.\n",
      "  Batch 3,200  of  23,521.    Elapsed: 0:21:50.\n",
      "  Batch 3,300  of  23,521.    Elapsed: 0:22:21.\n",
      "  Batch 3,400  of  23,521.    Elapsed: 0:22:52.\n",
      "  Batch 3,500  of  23,521.    Elapsed: 0:23:23.\n",
      "  Batch 3,600  of  23,521.    Elapsed: 0:23:54.\n",
      "  Batch 3,700  of  23,521.    Elapsed: 0:24:25.\n",
      "  Batch 3,800  of  23,521.    Elapsed: 0:24:55.\n",
      "  Batch 3,900  of  23,521.    Elapsed: 0:25:26.\n",
      "  Batch 4,000  of  23,521.    Elapsed: 0:25:57.\n",
      "  Batch 4,100  of  23,521.    Elapsed: 0:26:28.\n",
      "  Batch 4,200  of  23,521.    Elapsed: 0:26:59.\n",
      "  Batch 4,300  of  23,521.    Elapsed: 0:27:30.\n",
      "  Batch 4,400  of  23,521.    Elapsed: 0:28:01.\n",
      "  Batch 4,500  of  23,521.    Elapsed: 0:28:31.\n",
      "  Batch 4,600  of  23,521.    Elapsed: 0:29:02.\n",
      "  Batch 4,700  of  23,521.    Elapsed: 0:29:33.\n",
      "  Batch 4,800  of  23,521.    Elapsed: 0:30:04.\n",
      "  Batch 4,900  of  23,521.    Elapsed: 0:30:35.\n",
      "  Batch 5,000  of  23,521.    Elapsed: 0:31:06.\n",
      "  Batch 5,100  of  23,521.    Elapsed: 0:31:36.\n",
      "  Batch 5,200  of  23,521.    Elapsed: 0:32:07.\n",
      "  Batch 5,300  of  23,521.    Elapsed: 0:32:38.\n",
      "  Batch 5,400  of  23,521.    Elapsed: 0:33:09.\n",
      "  Batch 5,500  of  23,521.    Elapsed: 0:33:40.\n",
      "  Batch 5,600  of  23,521.    Elapsed: 0:34:11.\n",
      "  Batch 5,700  of  23,521.    Elapsed: 0:34:42.\n",
      "  Batch 5,800  of  23,521.    Elapsed: 0:35:13.\n",
      "  Batch 5,900  of  23,521.    Elapsed: 0:35:44.\n",
      "  Batch 6,000  of  23,521.    Elapsed: 0:36:15.\n",
      "  Batch 6,100  of  23,521.    Elapsed: 0:36:46.\n",
      "  Batch 6,200  of  23,521.    Elapsed: 0:37:18.\n",
      "  Batch 6,300  of  23,521.    Elapsed: 0:37:49.\n",
      "  Batch 6,400  of  23,521.    Elapsed: 0:38:20.\n",
      "  Batch 6,500  of  23,521.    Elapsed: 0:38:51.\n",
      "  Batch 6,600  of  23,521.    Elapsed: 0:39:22.\n",
      "  Batch 6,700  of  23,521.    Elapsed: 0:39:53.\n",
      "  Batch 6,800  of  23,521.    Elapsed: 0:40:24.\n",
      "  Batch 6,900  of  23,521.    Elapsed: 0:40:56.\n",
      "  Batch 7,000  of  23,521.    Elapsed: 0:41:27.\n",
      "  Batch 7,100  of  23,521.    Elapsed: 0:41:58.\n",
      "  Batch 7,200  of  23,521.    Elapsed: 0:42:29.\n",
      "  Batch 7,300  of  23,521.    Elapsed: 0:43:00.\n",
      "  Batch 7,400  of  23,521.    Elapsed: 0:43:31.\n",
      "  Batch 7,500  of  23,521.    Elapsed: 0:44:02.\n",
      "  Batch 7,600  of  23,521.    Elapsed: 0:44:33.\n",
      "  Batch 7,700  of  23,521.    Elapsed: 0:45:04.\n",
      "  Batch 7,800  of  23,521.    Elapsed: 0:45:36.\n",
      "  Batch 7,900  of  23,521.    Elapsed: 0:46:07.\n",
      "  Batch 8,000  of  23,521.    Elapsed: 0:46:38.\n",
      "  Batch 8,100  of  23,521.    Elapsed: 0:47:09.\n",
      "  Batch 8,200  of  23,521.    Elapsed: 0:47:40.\n",
      "  Batch 8,300  of  23,521.    Elapsed: 0:48:11.\n",
      "  Batch 8,400  of  23,521.    Elapsed: 0:48:42.\n",
      "  Batch 8,500  of  23,521.    Elapsed: 0:49:13.\n",
      "  Batch 8,600  of  23,521.    Elapsed: 0:49:44.\n",
      "  Batch 8,700  of  23,521.    Elapsed: 0:50:15.\n",
      "  Batch 8,800  of  23,521.    Elapsed: 0:50:46.\n",
      "  Batch 8,900  of  23,521.    Elapsed: 0:51:17.\n",
      "  Batch 9,000  of  23,521.    Elapsed: 0:51:48.\n",
      "  Batch 9,100  of  23,521.    Elapsed: 0:52:20.\n",
      "  Batch 9,200  of  23,521.    Elapsed: 0:52:51.\n",
      "  Batch 9,300  of  23,521.    Elapsed: 0:53:22.\n",
      "  Batch 9,400  of  23,521.    Elapsed: 0:53:53.\n",
      "  Batch 9,500  of  23,521.    Elapsed: 0:54:24.\n",
      "  Batch 9,600  of  23,521.    Elapsed: 0:54:55.\n",
      "  Batch 9,700  of  23,521.    Elapsed: 0:55:26.\n",
      "  Batch 9,800  of  23,521.    Elapsed: 0:55:57.\n",
      "  Batch 9,900  of  23,521.    Elapsed: 0:56:28.\n",
      "  Batch 10,000  of  23,521.    Elapsed: 0:56:59.\n",
      "  Batch 10,100  of  23,521.    Elapsed: 0:57:30.\n",
      "  Batch 10,200  of  23,521.    Elapsed: 0:58:02.\n",
      "  Batch 10,300  of  23,521.    Elapsed: 0:58:33.\n",
      "  Batch 10,400  of  23,521.    Elapsed: 0:59:04.\n",
      "  Batch 10,500  of  23,521.    Elapsed: 0:59:35.\n",
      "  Batch 10,600  of  23,521.    Elapsed: 1:00:06.\n",
      "  Batch 10,700  of  23,521.    Elapsed: 1:00:37.\n",
      "  Batch 10,800  of  23,521.    Elapsed: 1:01:08.\n",
      "  Batch 10,900  of  23,521.    Elapsed: 1:01:39.\n",
      "  Batch 11,000  of  23,521.    Elapsed: 1:02:10.\n",
      "  Batch 11,100  of  23,521.    Elapsed: 1:02:41.\n",
      "  Batch 11,200  of  23,521.    Elapsed: 1:03:12.\n",
      "  Batch 11,300  of  23,521.    Elapsed: 1:03:43.\n",
      "  Batch 11,400  of  23,521.    Elapsed: 1:04:15.\n",
      "  Batch 11,500  of  23,521.    Elapsed: 1:04:46.\n",
      "  Batch 11,600  of  23,521.    Elapsed: 1:05:17.\n",
      "  Batch 11,700  of  23,521.    Elapsed: 1:05:48.\n",
      "  Batch 11,800  of  23,521.    Elapsed: 1:06:19.\n",
      "  Batch 11,900  of  23,521.    Elapsed: 1:06:50.\n",
      "  Batch 12,000  of  23,521.    Elapsed: 1:07:21.\n",
      "  Batch 12,100  of  23,521.    Elapsed: 1:07:52.\n",
      "  Batch 12,200  of  23,521.    Elapsed: 1:08:23.\n",
      "  Batch 12,300  of  23,521.    Elapsed: 1:08:54.\n",
      "  Batch 12,400  of  23,521.    Elapsed: 1:09:25.\n",
      "  Batch 12,500  of  23,521.    Elapsed: 1:09:56.\n",
      "  Batch 12,600  of  23,521.    Elapsed: 1:10:28.\n",
      "  Batch 12,700  of  23,521.    Elapsed: 1:10:59.\n",
      "  Batch 12,800  of  23,521.    Elapsed: 1:11:30.\n",
      "  Batch 12,900  of  23,521.    Elapsed: 1:12:01.\n",
      "  Batch 13,000  of  23,521.    Elapsed: 1:12:32.\n",
      "  Batch 13,100  of  23,521.    Elapsed: 1:13:03.\n",
      "  Batch 13,200  of  23,521.    Elapsed: 1:13:34.\n",
      "  Batch 13,300  of  23,521.    Elapsed: 1:14:05.\n",
      "  Batch 13,400  of  23,521.    Elapsed: 1:14:36.\n",
      "  Batch 13,500  of  23,521.    Elapsed: 1:15:07.\n",
      "  Batch 13,600  of  23,521.    Elapsed: 1:15:38.\n",
      "  Batch 13,700  of  23,521.    Elapsed: 1:16:09.\n",
      "  Batch 13,800  of  23,521.    Elapsed: 1:16:40.\n",
      "  Batch 13,900  of  23,521.    Elapsed: 1:17:11.\n",
      "  Batch 14,000  of  23,521.    Elapsed: 1:17:42.\n",
      "  Batch 14,100  of  23,521.    Elapsed: 1:18:13.\n",
      "  Batch 14,200  of  23,521.    Elapsed: 1:18:44.\n",
      "  Batch 14,300  of  23,521.    Elapsed: 1:19:15.\n",
      "  Batch 14,400  of  23,521.    Elapsed: 1:19:46.\n",
      "  Batch 14,500  of  23,521.    Elapsed: 1:20:18.\n",
      "  Batch 14,600  of  23,521.    Elapsed: 1:20:49.\n",
      "  Batch 14,700  of  23,521.    Elapsed: 1:21:20.\n",
      "  Batch 14,800  of  23,521.    Elapsed: 1:21:51.\n",
      "  Batch 14,900  of  23,521.    Elapsed: 1:22:22.\n",
      "  Batch 15,000  of  23,521.    Elapsed: 1:22:53.\n",
      "  Batch 15,100  of  23,521.    Elapsed: 1:23:24.\n",
      "  Batch 15,200  of  23,521.    Elapsed: 1:23:55.\n",
      "  Batch 15,300  of  23,521.    Elapsed: 1:24:26.\n",
      "  Batch 15,400  of  23,521.    Elapsed: 1:24:57.\n",
      "  Batch 15,500  of  23,521.    Elapsed: 1:25:29.\n",
      "  Batch 15,600  of  23,521.    Elapsed: 1:26:00.\n",
      "  Batch 15,700  of  23,521.    Elapsed: 1:26:31.\n",
      "  Batch 15,800  of  23,521.    Elapsed: 1:27:02.\n",
      "  Batch 15,900  of  23,521.    Elapsed: 1:27:33.\n",
      "  Batch 16,000  of  23,521.    Elapsed: 1:28:04.\n",
      "  Batch 16,100  of  23,521.    Elapsed: 1:28:35.\n",
      "  Batch 16,200  of  23,521.    Elapsed: 1:29:06.\n",
      "  Batch 16,300  of  23,521.    Elapsed: 1:29:37.\n",
      "  Batch 16,400  of  23,521.    Elapsed: 1:30:08.\n",
      "  Batch 16,500  of  23,521.    Elapsed: 1:30:39.\n",
      "  Batch 16,600  of  23,521.    Elapsed: 1:31:10.\n",
      "  Batch 16,700  of  23,521.    Elapsed: 1:31:41.\n",
      "  Batch 16,800  of  23,521.    Elapsed: 1:32:13.\n",
      "  Batch 16,900  of  23,521.    Elapsed: 1:32:44.\n",
      "  Batch 17,000  of  23,521.    Elapsed: 1:33:15.\n",
      "  Batch 17,100  of  23,521.    Elapsed: 1:33:46.\n",
      "  Batch 17,200  of  23,521.    Elapsed: 1:34:17.\n",
      "  Batch 17,300  of  23,521.    Elapsed: 1:34:48.\n",
      "  Batch 17,400  of  23,521.    Elapsed: 1:35:19.\n",
      "  Batch 17,500  of  23,521.    Elapsed: 1:35:50.\n",
      "  Batch 17,600  of  23,521.    Elapsed: 1:36:21.\n",
      "  Batch 17,700  of  23,521.    Elapsed: 1:36:52.\n",
      "  Batch 17,800  of  23,521.    Elapsed: 1:37:23.\n",
      "  Batch 17,900  of  23,521.    Elapsed: 1:37:54.\n",
      "  Batch 18,000  of  23,521.    Elapsed: 1:38:26.\n",
      "  Batch 18,100  of  23,521.    Elapsed: 1:38:57.\n",
      "  Batch 18,200  of  23,521.    Elapsed: 1:39:28.\n",
      "  Batch 18,300  of  23,521.    Elapsed: 1:39:59.\n",
      "  Batch 18,400  of  23,521.    Elapsed: 1:40:30.\n",
      "  Batch 18,500  of  23,521.    Elapsed: 1:41:01.\n",
      "  Batch 18,600  of  23,521.    Elapsed: 1:41:32.\n",
      "  Batch 18,700  of  23,521.    Elapsed: 1:42:03.\n",
      "  Batch 18,800  of  23,521.    Elapsed: 1:42:35.\n",
      "  Batch 18,900  of  23,521.    Elapsed: 1:43:06.\n",
      "  Batch 19,000  of  23,521.    Elapsed: 1:43:37.\n",
      "  Batch 19,100  of  23,521.    Elapsed: 1:44:08.\n",
      "  Batch 19,200  of  23,521.    Elapsed: 1:44:39.\n",
      "  Batch 19,300  of  23,521.    Elapsed: 1:45:10.\n",
      "  Batch 19,400  of  23,521.    Elapsed: 1:45:41.\n",
      "  Batch 19,500  of  23,521.    Elapsed: 1:46:12.\n",
      "  Batch 19,600  of  23,521.    Elapsed: 1:46:43.\n",
      "  Batch 19,700  of  23,521.    Elapsed: 1:47:14.\n",
      "  Batch 19,800  of  23,521.    Elapsed: 1:47:46.\n",
      "  Batch 19,900  of  23,521.    Elapsed: 1:48:17.\n",
      "  Batch 20,000  of  23,521.    Elapsed: 1:48:48.\n",
      "  Batch 20,100  of  23,521.    Elapsed: 1:49:19.\n",
      "  Batch 20,200  of  23,521.    Elapsed: 1:49:50.\n",
      "  Batch 20,300  of  23,521.    Elapsed: 1:50:21.\n",
      "  Batch 20,400  of  23,521.    Elapsed: 1:50:52.\n",
      "  Batch 20,500  of  23,521.    Elapsed: 1:51:23.\n",
      "  Batch 20,600  of  23,521.    Elapsed: 1:51:54.\n",
      "  Batch 20,700  of  23,521.    Elapsed: 1:52:25.\n",
      "  Batch 20,800  of  23,521.    Elapsed: 1:52:57.\n",
      "  Batch 20,900  of  23,521.    Elapsed: 1:53:28.\n",
      "  Batch 21,000  of  23,521.    Elapsed: 1:53:59.\n",
      "  Batch 21,100  of  23,521.    Elapsed: 1:54:30.\n",
      "  Batch 21,200  of  23,521.    Elapsed: 1:55:01.\n",
      "  Batch 21,300  of  23,521.    Elapsed: 1:55:32.\n",
      "  Batch 21,400  of  23,521.    Elapsed: 1:56:03.\n",
      "  Batch 21,500  of  23,521.    Elapsed: 1:56:34.\n",
      "  Batch 21,600  of  23,521.    Elapsed: 1:57:05.\n",
      "  Batch 21,700  of  23,521.    Elapsed: 1:57:36.\n",
      "  Batch 21,800  of  23,521.    Elapsed: 1:58:07.\n",
      "  Batch 21,900  of  23,521.    Elapsed: 1:58:38.\n",
      "  Batch 22,000  of  23,521.    Elapsed: 1:59:09.\n",
      "  Batch 22,100  of  23,521.    Elapsed: 1:59:40.\n",
      "  Batch 22,200  of  23,521.    Elapsed: 2:00:11.\n",
      "  Batch 22,300  of  23,521.    Elapsed: 2:00:42.\n",
      "  Batch 22,400  of  23,521.    Elapsed: 2:01:14.\n",
      "  Batch 22,500  of  23,521.    Elapsed: 2:01:45.\n",
      "  Batch 22,600  of  23,521.    Elapsed: 2:02:16.\n",
      "  Batch 22,700  of  23,521.    Elapsed: 2:02:47.\n",
      "  Batch 22,800  of  23,521.    Elapsed: 2:03:18.\n",
      "  Batch 22,900  of  23,521.    Elapsed: 2:03:49.\n",
      "  Batch 23,000  of  23,521.    Elapsed: 2:04:20.\n",
      "  Batch 23,100  of  23,521.    Elapsed: 2:04:52.\n",
      "  Batch 23,200  of  23,521.    Elapsed: 2:05:23.\n",
      "  Batch 23,300  of  23,521.    Elapsed: 2:05:54.\n",
      "  Batch 23,400  of  23,521.    Elapsed: 2:06:26.\n",
      "  Batch 23,500  of  23,521.    Elapsed: 2:06:57.\n"
     ]
    }
   ],
   "source": [
    "sentences = rating_df.review_content\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 출력된 label 리스트\n",
    "label_list = list() \n",
    "\n",
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "# 문장을 입력 데이터로 변환\n",
    "inputs, masks = convert_input_data(tokenizer, sentences)\n",
    "\n",
    "# 데이터를 GPU에 넣음\n",
    "b_input_ids = inputs.to(device)\n",
    "b_input_mask = masks.to(device)\n",
    "\n",
    "test_data = TensorDataset(b_input_ids, b_input_mask)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    # 경과 정보 표시\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - start_time)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # 배치를 GPU에 넣음\n",
    "    batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "    # 배치에서 데이터 추출\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    preds = logits.detach().cpu().numpy()\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    label_list.append(list(pred_flat))\n",
    "\n",
    "# 이중 리스트를 단일 리스트로 변경\n",
    "result = list(itertools.chain.from_iterable(label_list))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e48636d5-5f9a-438d-9da6-848d14818925",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-4bf475bc22af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_sentence_many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"  Labeling took: {:}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-f9488f50a331>\u001b[0m in \u001b[0;36mtest_sentence_many\u001b[1;34m(model, device, tokenizer, sentences)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# 문장을 입력 데이터로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_input_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# 데이터를 GPU에 넣음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Jupyter_project\\keejun\\NLP_GNN_STUDY\\Movie\\utils.py\u001b[0m in \u001b[0;36mconvert_input_data\u001b[1;34m(tokenizer, sentences)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;31m# BERT의 토크나이저로 문장을 토큰으로 분리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m     \u001b[0mtokenized_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;31m# 입력 토큰의 최대 시퀀스 길이\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Jupyter_project\\keejun\\NLP_GNN_STUDY\\Movie\\utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;31m# BERT의 토크나이저로 문장을 토큰으로 분리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m     \u001b[0mtokenized_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;31m# 입력 토큰의 최대 시퀀스 길이\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch37\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch37\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[1;34m(tok_list, text)\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mtext_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             return list(\n\u001b[0m\u001b[0;32m    353\u001b[0m                 itertools.chain.from_iterable(\n\u001b[0;32m    354\u001b[0m                     (\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch37\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    353\u001b[0m                 itertools.chain.from_iterable(\n\u001b[0;32m    354\u001b[0m                     (\n\u001b[1;32m--> 355\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m                     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch37\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    228\u001b[0m                     \u001b[0msplit_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                     \u001b[0msplit_tokens\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0msplit_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch37\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    539\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m                         \u001b[0msubstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"##\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msubstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0msubstr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m                         \u001b[0mcur_substr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "labels = test_sentence_many(model, device, tokenizer, rating_df.review_content)\n",
    "print(\"  Labeling took: {:}\".format(format_time(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24b7488c-476b-4e72-98ad-9023c4dbc113",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02e373ef-def5-42f8-b803-e1b6a48a458f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "752664"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee9d43e3-1fe8-4b04-a471-c19eb61172c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-c54eb3c8a4ae>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rating_df['emotion'] = labels\n"
     ]
    }
   ],
   "source": [
    "rating_df['emotion'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8d1afe0-9ab3-46e4-970b-d77d4f86d7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_content</th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_date</th>\n",
       "      <th>critic_name</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher_name</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>943</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>Ben McEachen</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday Mail (Australia)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7242</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Harry Potter knockoffs don't come more transpa...</td>\n",
       "      <td>Rotten</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Nick Schager</td>\n",
       "      <td>False</td>\n",
       "      <td>Slant Magazine</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Percy Jackson isn't a great movie, but it's a ...</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Bill Goodykoontz</td>\n",
       "      <td>True</td>\n",
       "      <td>Arizona Republic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Fun, brisk and imaginative</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Jordan Hoffman</td>\n",
       "      <td>False</td>\n",
       "      <td>UGO</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4517</td>\n",
       "      <td>0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>Crammed with dragons, set-destroying fights an...</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Jim Schembri</td>\n",
       "      <td>True</td>\n",
       "      <td>The Age (Australia)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  review_score  \\\n",
       "0      943         0          0.70   \n",
       "1     7242         0          0.25   \n",
       "2     1046         0          0.70   \n",
       "3     4895         0          0.70   \n",
       "4     4517         0          0.60   \n",
       "\n",
       "                                      review_content review_type review_date  \\\n",
       "0  Whether audiences will get behind The Lightnin...       Fresh  2010-02-09   \n",
       "1  Harry Potter knockoffs don't come more transpa...      Rotten  2010-02-10   \n",
       "2  Percy Jackson isn't a great movie, but it's a ...       Fresh  2010-02-10   \n",
       "3                         Fun, brisk and imaginative       Fresh  2010-02-10   \n",
       "4  Crammed with dragons, set-destroying fights an...       Fresh  2010-02-10   \n",
       "\n",
       "        critic_name  top_critic           publisher_name  emotion  \n",
       "0      Ben McEachen       False  Sunday Mail (Australia)        2  \n",
       "1      Nick Schager       False           Slant Magazine        2  \n",
       "2  Bill Goodykoontz        True         Arizona Republic        2  \n",
       "3    Jordan Hoffman       False                      UGO        2  \n",
       "4      Jim Schembri        True      The Age (Australia)        2  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f558307-3be4-4d1a-bf9c-8c513e73ce6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 752664 entries, 0 to 752663\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   user_id         752664 non-null  int64  \n",
      " 1   movie_id        752664 non-null  int64  \n",
      " 2   review_score    752664 non-null  float64\n",
      " 3   review_content  752664 non-null  object \n",
      " 4   review_type     752664 non-null  object \n",
      " 5   review_date     752664 non-null  object \n",
      " 6   critic_name     752664 non-null  object \n",
      " 7   top_critic      752664 non-null  bool   \n",
      " 8   publisher_name  752664 non-null  object \n",
      " 9   emotion         752664 non-null  int64  \n",
      "dtypes: bool(1), float64(1), int64(3), object(5)\n",
      "memory usage: 52.4+ MB\n"
     ]
    }
   ],
   "source": [
    "rating_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "558a3e63-8b9e-4940-b155-de401129aec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                                                        4125\n",
       "movie_id                                                       2210\n",
       "review_score                                                    0.7\n",
       "review_content    It's a helluva ride with more laughs than anti...\n",
       "review_type                                                   Fresh\n",
       "review_date                                              2018-09-14\n",
       "critic_name                                           Jared Mobarak\n",
       "top_critic                                                    False\n",
       "publisher_name                                          BuffaloVibe\n",
       "emotion                                                           2\n",
       "Name: 65471, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df.loc[65471]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a54e107-bce8-4cfd-9238-b7b0977a8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.to_excel('./../../data/rotten_tomato/rotten_rating_review_emotion_table.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc3c48cb-c163-45e5-b320-5946187f1b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df2 = pd.read_csv('./../../data/rotten_tomato/rotten_rating_review_sentiment_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dfc88bf9-5990-4e33-a2bf-9e53b2168d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 752664 entries, 0 to 752663\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   user_id         752664 non-null  int64  \n",
      " 1   movie_id        752664 non-null  int64  \n",
      " 2   review_score    752664 non-null  float64\n",
      " 3   review_content  752664 non-null  object \n",
      " 4   review_type     752664 non-null  object \n",
      " 5   review_date     752664 non-null  object \n",
      " 6   critic_name     752664 non-null  object \n",
      " 7   top_critic      752664 non-null  bool   \n",
      " 8   publisher_name  752664 non-null  object \n",
      " 9   sentiment       752664 non-null  int64  \n",
      "dtypes: bool(1), float64(1), int64(3), object(5)\n",
      "memory usage: 52.4+ MB\n"
     ]
    }
   ],
   "source": [
    "rating_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "982d0899-b664-4d0a-a86e-e9ce0fb971ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df2.to_excel('./../../data/rotten_tomato/rotten_rating_review_sentiment_table.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ff22f-871b-4b3b-bd31-82cb2cdae624",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Pretraining 층으로만 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585187e-a749-4790-884f-11ba6e86a529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_no_finetuning = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "# model_no_finetuning.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8a0a2-c26d-4580-8cac-f6d221d9fe4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #시작 시간 설정\n",
    "# start_time = time.time()\n",
    "\n",
    "# # 평가모드로 변경\n",
    "# model_rotten2.eval()\n",
    "\n",
    "# # 변수 초기화\n",
    "# eval_loss, eval_accuracy = 0, 0\n",
    "# nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "# for step, batch in enumerate(test_dataloader):\n",
    "#     # 경과 정보 표시\n",
    "#     if step % 100 == 0 and not step == 0:\n",
    "#         elapsed = format_time(time.time() - start_time)\n",
    "#         print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "#     # 배치를 GPU에 넣음\n",
    "#     batch = tuple(b.to(device) for b in batch)\n",
    "    \n",
    "#     # 배치에서 데이터 추출\n",
    "#     b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "#     # 그래디언트 계산 안함\n",
    "#     with torch.no_grad():     \n",
    "#         # Forward 수행\n",
    "#         outputs = model_rotten2(b_input_ids, \n",
    "#                         token_type_ids=None, \n",
    "#                         attention_mask=b_input_mask)\n",
    "    \n",
    "#     # 로스 구함\n",
    "#     logits = outputs[0]\n",
    "\n",
    "#     # CPU로 데이터 이동\n",
    "#     logits = logits.detach().cpu().numpy()\n",
    "#     label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "#     # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "#     eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Accuracy: {0:.2f}\".format(eval_accuracy/len(test_dataloader)))\n",
    "# print(\"Test took: {:}\".format(format_time(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb764a0-bbb7-4fc5-9ace-8326b9084a33",
   "metadata": {},
   "source": [
    "---\n",
    "### 8. 특정 영화에 대한 긍정/부정 리뷰 취합 후 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8fa982f-5a4c-47a3-a968-92539d328654",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(path+'rotten_review_scaled_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a0efece-aaf1-4a64-9a28-ad8bd8290cee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>critic_name</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher_name</th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ben McEachen</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday Mail (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Nick Schager</td>\n",
       "      <td>False</td>\n",
       "      <td>Slant Magazine</td>\n",
       "      <td>Rotten</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Harry Potter knockoffs don't come more transpa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Bill Goodykoontz</td>\n",
       "      <td>True</td>\n",
       "      <td>Arizona Republic</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Percy Jackson isn't a great movie, but it's a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Jordan Hoffman</td>\n",
       "      <td>False</td>\n",
       "      <td>UGO</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>Fun, brisk and imaginative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Mark Adams</td>\n",
       "      <td>False</td>\n",
       "      <td>Daily Mirror (UK)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>This action-packed fantasy adventure, based on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rotten_tomatoes_link       critic_name  top_critic           publisher_name  \\\n",
       "0            m/0814255      Ben McEachen       False  Sunday Mail (Australia)   \n",
       "1            m/0814255      Nick Schager       False           Slant Magazine   \n",
       "2            m/0814255  Bill Goodykoontz        True         Arizona Republic   \n",
       "3            m/0814255    Jordan Hoffman       False                      UGO   \n",
       "4            m/0814255        Mark Adams       False        Daily Mirror (UK)   \n",
       "\n",
       "  review_type  review_score review_date  \\\n",
       "0       Fresh          1.00  2010-02-09   \n",
       "1      Rotten          0.25  2010-02-10   \n",
       "2       Fresh          1.00  2010-02-10   \n",
       "3       Fresh          0.70  2010-02-10   \n",
       "4       Fresh          0.80  2010-02-10   \n",
       "\n",
       "                                      review_content  label  \n",
       "0  Whether audiences will get behind The Lightnin...      1  \n",
       "1  Harry Potter knockoffs don't come more transpa...      0  \n",
       "2  Percy Jackson isn't a great movie, but it's a ...      1  \n",
       "3                         Fun, brisk and imaginative      1  \n",
       "4  This action-packed fantasy adventure, based on...      1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4732b5f-24c7-479b-ad60-d5f7451bb0ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_review = df[['rotten_tomatoes_link','review_content', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8616a9cf-0bad-42f9-9d4f-1ef06a45da4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "73d5b07d-0fad-4417-bcb5-dd42a17ea9ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pivot_review = df_review.pivot_table(index='rotten_tomatoes_link', columns='label', aggfunc=len, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c362eef3-ca3d-42e1-ae60-6487366e2a77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">review_content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>m/+_one_2019</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m/+h</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m/-_man</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m/-cule_valley_of_the_lost_ants</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m/0814255</th>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m/zoom_2006</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m/zootopia</th>\n",
       "      <td>2</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m/zorba_the_greek</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m/zulu</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m/zulu_dawn</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16933 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                review_content     \n",
       "label                                        0    1\n",
       "rotten_tomatoes_link                               \n",
       "m/+_one_2019                                 0   33\n",
       "m/+h                                         2    2\n",
       "m/-_man                                      1    3\n",
       "m/-cule_valley_of_the_lost_ants              0    5\n",
       "m/0814255                                   12   34\n",
       "...                                        ...  ...\n",
       "m/zoom_2006                                 22    3\n",
       "m/zootopia                                   2  166\n",
       "m/zorba_the_greek                            0    3\n",
       "m/zulu                                       0    4\n",
       "m/zulu_dawn                                  0    3\n",
       "\n",
       "[16933 rows x 2 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8d1a985e-9f24-4102-8806-64213ae930c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                label\n",
       "review_content  0        12\n",
       "                1        34\n",
       "Name: m/0814255, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot_review.loc['m/0814255']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "db134566-29a3-4856-89db-ba490ba0bb6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                label\n",
       "review_content  0        48\n",
       "                1        12\n",
       "Name: m/10000_bc, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot_review.loc['m/10000_bc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c75839b-ae10-4c73-b238-96ebe59d3b8e",
   "metadata": {},
   "source": [
    "- Movie1: 'm/0814255'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "87a5547a-85dd-4f14-a25a-29e9675f60cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 긍정, 부정 리뷰들을 취합 후 파일저장\n",
    "# def movie_sentiment_filter(df, movie_link, file_name):\n",
    "#     condition = df['rotten_tomatoes_link'] == movie_link\n",
    "#     df = df[condition]\n",
    "    \n",
    "#     pos_review_list = df[df['label'] == 1].reset_index(drop=True).review_content\n",
    "#     movie1_pos = \" \".join(pos_review_list)\n",
    "    \n",
    "#     neg_review_list = df[df['label'] == 0].reset_index(drop=True).review_content\n",
    "#     movie1_neg = \" \".join(neg_review_list)\n",
    "    \n",
    "#     f = open(path + f\"{file_name}_pos.txt\", 'w')\n",
    "#     f.write(movie1_pos)\n",
    "#     f.close()\n",
    "    \n",
    "#     f = open(path + f\"{file_name}_neg.txt\", 'w')\n",
    "#     f.write(movie1_neg)\n",
    "#     f.close()\n",
    "    \n",
    "#     print(f\"{file_name}_pos/neg save finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a3b07188-91f8-489e-ac06-d0a4003ef8d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie1_pos/neg save finish!!\n"
     ]
    }
   ],
   "source": [
    "# movie_sentiment_filter(df=df_review, movie_link='m/0814255', file_name='movie1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e825ec1e-ad86-4639-8bcb-093b3f797eec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie2_pos/neg save finish!!\n"
     ]
    }
   ],
   "source": [
    "# movie_sentiment_filter(df=df_review, movie_link='m/0878835', file_name='movie2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d912b320-11fc-4aac-b174-8aba8a4cfdbe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie3_pos/neg save finish!!\n"
     ]
    }
   ],
   "source": [
    "# movie_sentiment_filter(df=df_review, movie_link='m/10000_bc', file_name='movie3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b536f-2777-46eb-a04b-23204028994d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
