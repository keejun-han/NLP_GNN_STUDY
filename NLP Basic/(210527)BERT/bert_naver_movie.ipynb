{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT 이용 네이버 영화 리뷰 감성분석\n",
    "\n",
    "- 발표일 : 2021/05/26\n",
    "- 작성자 : Soyoung Cho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 3)\n",
      "(50000, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/ratings_train.txt\", sep='\\t')\n",
    "test = pd.read_csv(\"data/ratings_test.txt\", sep='\\t')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "탭을 이용해 구분하여 데이터프레임으로 읽어옵니다. 학습 데이터 75%, 테스트 데이터 25%의 데이터가 구분되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Input representation\n",
    "\n",
    "![](https://blog.kakaocdn.net/dn/WFCfe/btqBWZ40Gmc/6FkuwsAGN9e7Uudmi03k4k/img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT에서는 문장 앞에 [CLS]라는 스페셜 토큰을 붙여 문장의 시작임을 알립니다.\n",
    "  - [CLS] 심볼은 Classification을 뜻합니다. Fine-tuning시 출력에서 이 위치의 값을 사용하여 분류를 합니다.\n",
    "- 문장 종료는 [SEP]라는 스페셜 토큰으로 모델에게 알립니다.\n",
    "  - [SEP] 심볼은 Seperation을 뜻합니다. 두 문장을 구분하는 역할을 합니다.\n",
    "  - 이 예제에서는 문장이 하나이므로 [SEP]도 하나만 넣습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"[CLS] \" + str(document) + \" [SEP]\" for document in train.document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 아 더빙.. 진짜 짜증나네요 목소리 [SEP]',\n",
       " '[CLS] 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 [SEP]',\n",
       " '[CLS] 너무재밓었다그래서보는것을추천한다 [SEP]',\n",
       " '[CLS] 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정 [SEP]',\n",
       " '[CLS] 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다 [SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT는 형태소분석으로 토큰을 분리하지 않습니다. **WordPiece**라는 통계적인 방식을 사용합니다.\n",
    "- 한 단어 내에서 자주 나오는 글자들을 붙여서 하나의 토큰으로 만듭니다. 이렇게 하면 언어에 상관없이 토큰을 생성할 수 있다는 장점이 있습니다. 또한 신조어 같이 사전에 없는 단어(OOV)를 처리하기도 좋습니다. \n",
    "- 토크나이저는 여러 언어의 데이터를 기반으로 만든 'bert-base-multilingual-cased'를 사용합니다. 그래서 한글도 처리가 가능합니다.\n",
    "- 아래의 결과에서 ## 기호는 앞 토큰과 이어진다는 표시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라벨 추출\n",
    "labels = train['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '아', '더', '##빙', '.', '.', '진', '##짜', '짜', '##증', '##나', '##네', '##요', '목', '##소', '##리', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(s) for s in sentences]\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 딥러닝 모델에는 토큰 자체를 입력으로 넣을 수 없습니다. 임베딩 레이어에는 토큰을 숫자로 된 인덱스로 변환하여 사용해야 합니다. BERT의 토크나이저는 {단어토큰:인덱스}로 구성된 단어사전을 가지고 있습니다. 이를 참조하여 토큰을 인덱스로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128 # 입력 토큰의 최대 시퀀스 길이\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts] # 토큰을 숫자 인덱스로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9519, 9074, 119005, 119, 119, 9708, 119235, 9715, 119230, 16439, 77884, 48549, 9284, 22333, 12692, 102]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 문장씩 문장별 토큰을 각각의 숫자 ID로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 문장의 첫 번째 토큰 :  [CLS]\n",
      "두 번째 문장의 첫 번째 토큰 :  [CLS]\n",
      "첫 번째 문장의 마지막 토큰 :  [SEP]\n",
      "두 번째 문장의 마지막 토큰 :  [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(\"첫 번째 문장의 첫 번째 토큰 : \", tokenized_texts[0][0])\n",
    "print(\"두 번째 문장의 첫 번째 토큰 : \", tokenized_texts[1][0])\n",
    "print(\"첫 번째 문장의 마지막 토큰 : \", tokenized_texts[0][-1])\n",
    "print(\"두 번째 문장의 마지막 토큰 : \", tokenized_texts[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 문장의 첫 번째 토큰 id :  101\n",
      "두 번째 문장의 첫 번째 토큰 id :  101\n",
      "첫 번째 문장의 마지막 토큰 id :  102\n",
      "두 번째 문장의 마지막 토큰 id :  102\n"
     ]
    }
   ],
   "source": [
    "print(\"첫 번째 문장의 첫 번째 토큰 id : \", input_ids[0][0])\n",
    "print(\"두 번째 문장의 첫 번째 토큰 id : \", input_ids[1][0])\n",
    "print(\"첫 번째 문장의 마지막 토큰 id : \", input_ids[0][-1])\n",
    "print(\"두 번째 문장의 마지막 토큰 id : \", input_ids[1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일한 토큰은 동일 id를 가지는 것을 볼 수 있습니다. CLS 토큰은 101의 ID, SEP 토큰은 102의 ID를 갖네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,   9519,   9074, 119005,    119,    119,   9708, 119235,\n",
       "         9715, 119230,  16439,  77884,  48549,   9284,  22333,  12692,\n",
       "          102,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스의 함수 pad_sequences를 이용하여 문장에서 남는 부분을 패딩 0으로 채워줍니다. pad_sequences 함수는 문장의 길이를 맞출 수 있도록 패딩해주는 함수입니다.\n",
    "인자에 대해 간단히 살펴보면 다음과 같습니다.\n",
    "- sequences : 시퀀스 리스트이며, 각 시퀀스는 integer 리스트입니다.\n",
    "- maxlen : 시퀀스의 최대 길이입니다. 시퀀스가 이 최대 길이를 초과하면 시퀀스를 자릅니다. maxlen을 명시하지 않으면 가장 긴 시퀀스에 맞춰집니다.\n",
    "- truncating: 시퀀스가 maxlen을 초과하는 경우, 초과하는 값을 앞에서 자를지, 뒤에서 잘을지 명시합니다. defaults to 'pre'.\n",
    "- padding : 패딩을 앞에 채울지, 뒤에 채울지 의미합니다.우리는 시퀀스 뒤에 패딩하기 위해 'post'라고 적어줍니다. defaults to 'pre'.\n",
    "- value : 패딩을 채울 값이고, default는 0입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞에서 패딩은 0으로 했었기에, 0보다 큰지 작은지를 비교하여 패딩과 유의미한 토큰을 구분할 수 있습니다.\n",
    "- 패딩을 마스크하는 목적은 학습속도를 높이기 위함입니다.\n",
    "- PAD 토큰은 실질적인 의미를 가진 토큰이 아니기에, 이에 대해서는 연산을 하지 않도록 마스킹(Masking)을 하여 어텐션에서 제외하도록 하는 것입니다.\n",
    "- 이렇게 실 데이터가 있는 곳과 padding이 있는 곳을 attention에게 알려줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 마스크 초기화\n",
    "attention_masks = []\n",
    "\n",
    "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq] # 패딩은 0으로 했었음. 그 외 숫자는 0보다 큼. True일 경우 1.\n",
    "    attention_masks.append(seq_mask) # 각 시퀀스 마스크 결과 append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train , valid set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train 데이터를 train set과 valid set으로 구분합니다.\n",
    "- input과 mask가 뒤섞이지 않도록 random_state를 일정하게 고정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input id, label를 train과 valid로 분리\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels, \n",
    "                                                                                    random_state=42, \n",
    "                                                                                    test_size=0.1)\n",
    "\n",
    "# 어텐션 마스크를 train과 valid로 분리\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=42, \n",
    "                                                       test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to torch.tenser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 파이토치의 텐서로 변환\n",
    "train_inputs = torch.tensor(train_inputs) # 시퀀스를 토큰 ID로 표현\n",
    "train_labels = torch.tensor(train_labels) # 긍/부정\n",
    "train_masks = torch.tensor(train_masks) # 패딩 마스크 (attention mask)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_inputs[0])\n",
    "# print(train_labels[0])\n",
    "# print(train_masks[0])\n",
    "\n",
    "# print(validation_inputs[0])\n",
    "# print(validation_labels[0])\n",
    "# print(validation_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 및 데이터로더 설정\n",
    "현재 쓰고 있는 GPU의 VRAM에 맞도록 배치사이즈를 설정합니다.\n",
    "우선 배치사이즈를 크게 넣어보고 VRAM 부족 메시지가 나오면 8의 배수 중 더 작은 것으로 줄여나가는 것이 일반적인 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "# 학습시 배치 사이즈만큼 데이터를 가져옵니다.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TensorDataset 클래스 : Dataset을 상속한 클래스로,  학습 데이터 X와 레이블 Y를 묶어 놓는 컨테이너입니다. TensorDataset을 DataLoader에 전달하면 for 루프에서 데이터의 일부분만 간단히 추출할 수 있습니다. TensorDataset에는 텐서만 전달할 수 있으며, Variable은 전달할 수 없으니 주의합니다. (출처 : https://truman.tistory.com/223)\n",
    "\n",
    "- RandomSampler : 랜덤하게 데이터를 추출합니다.\n",
    "\n",
    "- DataLoader 클래스 : 데이터셋을 배치 단위로 쪼개어 학습 시 반복문 안에서 데이터를 공급해 데이터를 쉽게 로드할 수 있습니다.\n",
    "\n",
    "- SequentialSampler 클래스 : 데이터를 순차적으로 추출합니다. Always in the same order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test셋 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 train, valid 데이터셋 전처리와 동일합니다.\n",
    "- train 전처리시 함께 처리해도 무방합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"[CLS] \" + str(document) + \" [SEP]\" for document in test.document]\n",
    "labels = test['label'].values\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU 체크 및 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device 0 : NVIDIA GeForce RTX 3080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\torch37\\lib\\site-packages\\torch\\cuda\\__init__.py:106: UserWarning: \n",
      "NVIDIA GeForce RTX 3080 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 3080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "\n",
    "n_devices = torch.cuda.device_count()\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    for i in range(n_devices):\n",
    "        print(\"Device\",i,\":\", torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류를 위한 BERT 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Huggning Face가 제공하는 BertForSequenceClassification() 함수를 이용하여 쉽게 구현할 수 있습니다.\n",
    "- 이진분류이기 때문에 num_labels는 2로 둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.mccormickml.com/assets/BERT/padding_and_mask.png)\n",
    "\n",
    "Pre-training을 마친 BERT는 다양한 문제로 전이학습(Fine-tuning)이 가능합니다. 여기서는 위의 그림과 같이 한 문장을 분류하는 방법을 사용합니다. 영화리뷰 문장이 입력으로 들어가면, 긍정/부정으로 구분합니다. 모델의 출력에서 [CLS] 위치인 첫 번째 토큰에 새로운 레이어를 붙여서 파인튜닝을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # 학습률\n",
    "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "                )\n",
    "\n",
    "# 에폭수\n",
    "epochs = 2\n",
    "\n",
    "# 총 훈련 스텝 = 배치반복 횟수 * 에폭\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# lr 조금씩 감소시키는 스케줄러\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "attachments": {
    "033ee147-cc02-4520-ad5a-645241b391bf.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transformers에서 제공하는 옵티마이저 중 AdamW를 사용합니다.\n",
    "- Learning Rate Scheduler는 transformers에서 제공하는 것을 사용합니다.\n",
    "\n",
    "![image.png](attachment:033ee147-cc02-4520-ad5a-645241b391bf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch, Epoch, Step 개념 복습\n",
    "- Batch : 한 번에 몇 개의 데이터씩 처리할지. mini-batch 그 개념. 병렬적으로 처리되는 각 데이터 뭉탱이.\n",
    "- Epoch : 전체 데이터에 대해 한 바퀴 도는 것. 일반적으로 1 에폭이 끝나면 데이터를 랜덤으로 섞는다.\n",
    "- Step : 한 배치가 도는 것. 1 batch 도는 것 = 1 step. len(data_loader)를 하면 배치 개수를 구할 수 있다.\n",
    "  - Batch_size, epoch을 정해주면 step은 자동적으로 계산되기에 사실 hyperparameter는 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기타 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터로더에서 배치만큼 가져온 후 forward, backward pass를 수행합니다.\n",
    "- gradient update는 명시적으로 하지 않고, 위에서 로드한 optimizer를 활용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 파라미터\n",
    "- attention_mask : 패딩 토큰 인덱스에 attention을 수행하는 것을 방지하기 위해 마스킹한 인덱스들입니다\n",
    "  - 0과 1로 표기하며 마스킹 되지 않았다면 (패딩 토큰이 아닌 경우) 1, 패딩 토큰인 경우 0으로 합니다.\n",
    "- token_type_ids : 입력 시퀀스가 두 문장으로 구성될 경우 첫 번째 문장을 0으로, 두 번째 문장을 1로 명시해줍니다.\n",
    "  - 우리는 문장 1개만을 이용하므로 None이라 설정합니다.\n",
    "- labels : 사이즈는 (batch_size,)이며, 이를 이용해 시퀀스 loss를 계산합니다.\n",
    "  - labels 인덱스들은 클래스 개수에 따라 0부터 클래스개수-1의 숫자로 표현되어야 합니다.\n",
    "  - config.num_labels 가 1인 경우, regression loss가 계산되며 (Mean-Square loss), config.num_labels가 1보다 큰 경우, 즉 binary classification ~ multi-class classification인 경우 Cross-Entropy를 이용해 loss가 계산됩니다.\n",
    "  - 우리는 모델을 불러올 때 (model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    ") num_labels=2로 주었었습니다. 긍정/부정을 분류하는 이진 분류 문제이기 때문입니다. 따라서 크로스 엔트로피로 계산됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Clipping (그래디언트 클리핑)\n",
    "- 주로 RNN계열에서 gradient vanishing이나 gradient exploding이 많이 발생하는데, gradient exploding을 방지하여 학습의 안정화를 도모하기 위해 사용하는 방법입니다.\n",
    "- clipping이란 단어에서 유추할 수 있듯이 gradient가 일정 threshold를 넘어가면 clipping을 해줍니다. clipping은 gradient의 L2norm (norm이지만 보통 L2 norm 사용)으로 나눠주는 방식으로 하게 된다고 합니다.\n",
    "- 출처 : https://sanghyu.tistory.com/87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model.eval()\n",
    "- train 할 때와 evaluate 해야 할 때 다르게 동작해야 하는 레이어에는 대표적으로 Dropout Layer와 BatchNorm Layer가 있습니다. \n",
    "- .eval() 함수는 evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수라고 합니다.\n",
    "- evaluation/validation 과정에선 보통 model.eval()과 torch.no_grad()를 함께 사용합니다.\n",
    "- 출처 : https://bluehorn07.github.io/2021/02/27/model-eval-and-train.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "Batch   500  of  4,219. Elapsed: 0:08:01.\n",
      "Batch 1,000  of  4,219. Elapsed: 0:16:03.\n",
      "Batch 1,500  of  4,219. Elapsed: 0:24:04.\n",
      "Batch 2,000  of  4,219. Elapsed: 0:32:04.\n",
      "Batch 2,500  of  4,219. Elapsed: 0:40:05.\n",
      "Batch 3,000  of  4,219. Elapsed: 0:48:05.\n",
      "Batch 3,500  of  4,219. Elapsed: 0:56:05.\n",
      "Batch 4,000  of  4,219. Elapsed: 1:04:05.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epcoh took: 1:07:35\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.86\n",
      "  Validation took: 0:02:25\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "Batch   500  of  4,219. Elapsed: 0:08:00.\n",
      "Batch 1,000  of  4,219. Elapsed: 0:16:00.\n",
      "Batch 1,500  of  4,219. Elapsed: 0:24:00.\n",
      "Batch 2,000  of  4,219. Elapsed: 0:32:34.\n",
      "Batch 2,500  of  4,219. Elapsed: 0:41:35.\n",
      "Batch 3,000  of  4,219. Elapsed: 0:50:47.\n",
      "Batch 3,500  of  4,219. Elapsed: 0:59:50.\n",
      "Batch 4,000  of  4,219. Elapsed: 1:08:30.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epcoh took: 1:12:04\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation took: 0:02:28\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# 에폭 수만큼 반복\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               1. Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    # 시작 시간 설정\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시 (step 500번마다 출력)\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - start_time)\n",
    "            print('Batch {:>5,}  of  {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        \n",
    "        # 배치를 GPU에 올림\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출 (input, mask, label 순으로 넣었었음)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # forward 수행\n",
    "        outputs = model(b_input_ids,\n",
    "                        attention_mask=b_input_mask,\n",
    "                       token_type_ids=None,\n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs.loss # outputs[0]\n",
    "        \n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward 수행으로 그래디언트 계산 (Back-propagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0) # 예제 코드에서는 1.0이었음\n",
    "        \n",
    "        # 그래디언트를 이용해 가중치 파라미터를 lr만큼 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 그래디언트 초기화\n",
    "        ## (호출시 경사값을 0으로 설정. 이유 : 반복 때마다 기울기를 새로 계산하기 때문)\n",
    "        model.zero_grad()\n",
    "    \n",
    "    # 1 에폭이 끝나면 평균 train 로스 계산 (전체 loss / 배치 수)\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - start_time)))\n",
    "    \n",
    "    # ========================================\n",
    "    #               2. Validation\n",
    "    # ========================================\n",
    "    \n",
    "    # 1 에폭이 끝나면 validation 시행\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    \n",
    "    # 시작 시간 설정\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 평가 모드로 변경\n",
    "    model.eval()\n",
    "    \n",
    "    # 변수 초기화\n",
    "    total_valid_accuracy = 0\n",
    "    nb_valid_steps = 0\n",
    "    \n",
    "    # valid 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # 그래디언트 계산 안함!\n",
    "        with torch.no_grad():\n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # 로스 구함 (train할 때는 loss, validation할 때는 logits)\n",
    "        ## logits은 softmax를 거치기 전의 classification score를 반환합니다. shape: (batch_size, config.num_labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        valid_accuracy = flat_accuracy(logits, label_ids)\n",
    "        total_valid_accuracy += valid_accuracy\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(total_valid_accuracy/len(validation_dataloader)))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - start_time)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. 모델 테스트 (test set 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,563.    Elapsed: 0:01:58.\n",
      "  Batch   200  of  1,563.    Elapsed: 0:02:29.\n",
      "  Batch   300  of  1,563.    Elapsed: 0:03:00.\n",
      "  Batch   400  of  1,563.    Elapsed: 0:03:31.\n",
      "  Batch   500  of  1,563.    Elapsed: 0:04:01.\n",
      "  Batch   600  of  1,563.    Elapsed: 0:04:32.\n",
      "  Batch   700  of  1,563.    Elapsed: 0:05:03.\n",
      "  Batch   800  of  1,563.    Elapsed: 0:05:34.\n",
      "  Batch   900  of  1,563.    Elapsed: 0:06:05.\n",
      "  Batch 1,000  of  1,563.    Elapsed: 0:06:36.\n",
      "  Batch 1,100  of  1,563.    Elapsed: 0:07:07.\n",
      "  Batch 1,200  of  1,563.    Elapsed: 0:07:39.\n",
      "  Batch 1,300  of  1,563.    Elapsed: 0:08:10.\n",
      "  Batch 1,400  of  1,563.    Elapsed: 0:08:41.\n",
      "  Batch 1,500  of  1,563.    Elapsed: 0:09:12.\n",
      "\n",
      "Accuracy: 0.87\n",
      "Test took: 0:09:31\n"
     ]
    }
   ],
   "source": [
    "#시작 시간 설정\n",
    "start_time = time.time()\n",
    "\n",
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "# 변수 초기화\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    # 경과 정보 표시\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - start_time)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # 배치를 GPU에 넣음\n",
    "    batch = tuple(b.to(device) for b in batch)\n",
    "    \n",
    "    # 배치에서 데이터 추출\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    \n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/len(test_dataloader)))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4. 새로운 문장 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 변환\n",
    "def convert_input_data(sentences):\n",
    "\n",
    "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # 입력 토큰의 최대 시퀀스 길이\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # 토큰을 숫자 인덱스로 변환\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    \n",
    "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # 어텐션 마스크 초기화\n",
    "    attention_masks = []\n",
    "\n",
    "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # 데이터를 파이토치의 텐서로 변환\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 테스트\n",
    "def test_sentences(sentences):\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 문장을 입력 데이터로 변환\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "\n",
    "    # 데이터를 GPU에 넣음\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "            \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2956648  1.3821511]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['연기는 별로지만 재미 하나는 끝내줌!'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.9581914 -3.235781 ]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['주연배우가 아깝다. 총체적 난국...'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.720135  -2.9258687]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['주연배우가 아깝다. 영화노잼...'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://yonghee.io/bert_binary_classification_naver/\n",
    " - https://colab.research.google.com/drive/1tIf0Ugdqg4qT7gcxia3tL7und64Rv1dP#scrollTo=I3vlyUJuVRo5  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
